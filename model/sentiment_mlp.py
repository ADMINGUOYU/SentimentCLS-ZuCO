"""
MLP-based sentiment classifier that uses embeddings from GLIM.

This model takes the EEG embeddings generated by GLIM and uses a simple
Multi-Layer Perceptron (MLP) to predict sentiment labels.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import lightning as L
from torchmetrics.functional.classification import multiclass_accuracy


class SentimentMLP(L.LightningModule):
    """
    Simple MLP classifier for sentiment classification using GLIM embeddings.
    
    Args:
        input_dim: Dimension of input embeddings from GLIM (default: 1024)
        hidden_dims: List of hidden layer dimensions (default: [512, 256])
        num_classes: Number of sentiment classes (default: 3 for negative/neutral/positive)
        dropout: Dropout rate for regularization (default: 0.3)
        lr: Learning rate (default: 1e-3)
        weight_decay: Weight decay for optimizer (default: 1e-4)
    """
    
    def __init__(
        self,
        input_dim: int = 1024,
        hidden_dims: list = None,
        num_classes: int = 3,
        dropout: float = 0.3,
        lr: float = 1e-3,
        weight_decay: float = 1e-4,
    ):
        super().__init__()
        
        if hidden_dims is None:
            hidden_dims = [512, 256]
        
        self.input_dim = input_dim
        self.hidden_dims = hidden_dims
        self.num_classes = num_classes
        self.lr = lr
        self.weight_decay = weight_decay
        
        # Define sentiment labels
        self.sentiment_labels = ['negative', 'neutral', 'positive']
        
        # Build MLP layers
        layers = []
        in_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(in_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            in_dim = hidden_dim
        
        # Output layer
        layers.append(nn.Linear(in_dim, num_classes))
        
        self.mlp = nn.Sequential(*layers)
        
        # Save hyperparameters
        self.save_hyperparameters()
    
    def forward(self, embeddings):
        """
        Forward pass through the MLP.
        
        Args:
            embeddings: Input embeddings of shape (batch_size, input_dim)
            
        Returns:
            logits: Output logits of shape (batch_size, num_classes)
        """
        return self.mlp(embeddings)
    
    def training_step(self, batch, batch_idx):
        """Training step."""
        embeddings = batch['eeg_emb_vector']  # (batch_size, embed_dim)
        sentiment_ids = batch['sentiment_ids']  # (batch_size,)
        
        # Forward pass
        logits = self(embeddings)
        
        # Calculate loss
        loss = F.cross_entropy(logits, sentiment_ids, ignore_index=-1)
        
        # Calculate accuracy
        probs = F.softmax(logits, dim=-1)
        acc = multiclass_accuracy(probs, sentiment_ids, 
                                  average='micro', 
                                  num_classes=self.num_classes, 
                                  ignore_index=-1,
                                  top_k=1)
        
        # Log metrics
        self.log('train/loss', loss, prog_bar=True, sync_dist=True)
        self.log('train/accuracy', acc, prog_bar=True, sync_dist=True)
        
        return loss
    
    def validation_step(self, batch, batch_idx):
        """Validation step."""
        embeddings = batch['eeg_emb_vector']
        sentiment_ids = batch['sentiment_ids']
        
        # Forward pass
        logits = self(embeddings)
        
        # Calculate loss
        loss = F.cross_entropy(logits, sentiment_ids, ignore_index=-1)
        
        # Calculate accuracy
        probs = F.softmax(logits, dim=-1)
        acc = multiclass_accuracy(probs, sentiment_ids,
                                  average='micro',
                                  num_classes=self.num_classes,
                                  ignore_index=-1,
                                  top_k=1)
        
        # Log metrics
        self.log('val/loss', loss, prog_bar=True, sync_dist=True)
        self.log('val/accuracy', acc, prog_bar=True, sync_dist=True)
        
        return loss
    
    def test_step(self, batch, batch_idx):
        """Test step."""
        embeddings = batch['eeg_emb_vector']
        sentiment_ids = batch['sentiment_ids']
        
        # Forward pass
        logits = self(embeddings)
        
        # Calculate loss
        loss = F.cross_entropy(logits, sentiment_ids, ignore_index=-1)
        
        # Calculate accuracy
        probs = F.softmax(logits, dim=-1)
        acc = multiclass_accuracy(probs, sentiment_ids,
                                  average='micro',
                                  num_classes=self.num_classes,
                                  ignore_index=-1,
                                  top_k=1)
        
        # Get predictions
        preds = torch.argmax(probs, dim=1)
        
        # Log metrics
        self.log('test/loss', loss, sync_dist=True)
        self.log('test/accuracy', acc, sync_dist=True)
        
        return {'loss': loss, 'accuracy': acc, 'predictions': preds, 'targets': sentiment_ids}
    
    def predict_step(self, batch, batch_idx):
        """Prediction step."""
        embeddings = batch['eeg_emb_vector']
        
        # Forward pass
        logits = self(embeddings)
        probs = F.softmax(logits, dim=-1)
        preds = torch.argmax(probs, dim=1)
        
        # Convert predictions to labels
        pred_labels = [self.sentiment_labels[pred.item()] for pred in preds]
        
        return {'predictions': preds, 'probabilities': probs, 'labels': pred_labels}
    
    def configure_optimizers(self):
        """Configure optimizer."""
        optimizer = torch.optim.Adam(
            self.parameters(),
            lr=self.lr,
            weight_decay=self.weight_decay
        )
        
        # Learning rate scheduler
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='min',
            factor=0.5,
            patience=5,
            verbose=True
        )
        
        return {
            'optimizer': optimizer,
            'lr_scheduler': {
                'scheduler': scheduler,
                'monitor': 'val/loss',
                'interval': 'epoch',
                'frequency': 1
            }
        }
