"""
MLP-based sentiment classifier that uses embeddings from GLIM.

This model takes the EEG embeddings generated by GLIM and uses a simple
Multi-Layer Perceptron (MLP) to predict sentiment labels.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import lightning as L
from torchmetrics.functional.classification import multiclass_accuracy
from sklearn.metrics import confusion_matrix
import numpy as np


class SentimentMLP(L.LightningModule):
    """
    Simple MLP classifier for sentiment classification using GLIM embeddings.
    
    Args:
        input_dim: Dimension of input embeddings from GLIM (default: 1024)
        hidden_dims: List of hidden layer dimensions (default: [512, 256])
        num_classes: Number of sentiment classes (default: 3 for negative/neutral/positive)
        dropout: Dropout rate for regularization (default: 0.3)
        lr: Learning rate (default: 1e-3)
        weight_decay: Weight decay for optimizer (default: 1e-4)
    """
    
    def __init__(
        self,
        input_dim: int = 1024,
        hidden_dims: list = None,
        num_classes: int = 3,
        dropout: float = 0.3,
        lr: float = 1e-3,
        weight_decay: float = 1e-4,
    ):
        super().__init__()
        
        if hidden_dims is None:
            hidden_dims = [512, 256]
        
        self.input_dim = input_dim
        self.hidden_dims = hidden_dims
        self.num_classes = num_classes
        self.lr = lr
        self.weight_decay = weight_decay
        
        # Define sentiment labels
        self.sentiment_labels = ['negative', 'neutral', 'positive']
        
        # Initialize lists to accumulate training metrics per epoch
        self.training_step_outputs = []
        
        # Initialize lists to accumulate test predictions and targets for confusion matrix
        self.test_step_outputs = []
        
        # Build MLP layers
        layers = []
        in_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(in_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            in_dim = hidden_dim
        
        # Output layer
        layers.append(nn.Linear(in_dim, num_classes))
        
        self.mlp = nn.Sequential(*layers)
        
        # Save hyperparameters
        self.save_hyperparameters()
    
    def forward(self, embeddings):
        """
        Forward pass through the MLP.
        
        Args:
            embeddings: Input embeddings of shape (batch_size, input_dim)
            
        Returns:
            logits: Output logits of shape (batch_size, num_classes)
        """
        return self.mlp(embeddings)
    
    def training_step(self, batch, batch_idx):
        """Training step."""
        embeddings = batch['eeg_emb_vector']  # (batch_size, embed_dim)
        sentiment_ids = batch['sentiment_ids']  # (batch_size,)
        sentiment_ids = sentiment_ids.to(torch.int64) 
        
        # Forward pass
        logits = self(embeddings)
        
        # Calculate loss
        loss = F.cross_entropy(logits, sentiment_ids, ignore_index=-1)
        
        # Calculate accuracy (use logits directly, no need for softmax)
        acc = multiclass_accuracy(logits, sentiment_ids, 
                                  average='micro', 
                                  num_classes=self.num_classes, 
                                  ignore_index=-1,
                                  top_k=1)
        
        # Store metrics for epoch-level logging
        self.training_step_outputs.append({
            'loss': loss.detach(),
            'accuracy': acc.detach()
        })
        
        return loss
    
    def on_train_epoch_end(self):
        """Compute and log average metrics at the end of each training epoch."""
        if len(self.training_step_outputs) == 0:
            return
        
        # Compute average loss and accuracy
        avg_loss = torch.stack([x['loss'] for x in self.training_step_outputs]).mean()
        avg_acc = torch.stack([x['accuracy'] for x in self.training_step_outputs]).mean()
        
        # Log to tensorboard
        self.log('train/loss', avg_loss, prog_bar=True, sync_dist=True)
        self.log('train/accuracy', avg_acc, prog_bar=True, sync_dist=True)
        
        # Clear for next epoch
        self.training_step_outputs.clear()
    
    def validation_step(self, batch, batch_idx):
        """Validation step."""
        embeddings = batch['eeg_emb_vector']
        sentiment_ids = batch['sentiment_ids']
        sentiment_ids = sentiment_ids.to(torch.int64) 
        
        # Forward pass
        logits = self(embeddings)
        
        # Calculate loss
        loss = F.cross_entropy(logits, sentiment_ids, ignore_index=-1)
        
        # Calculate accuracy (use logits directly, no need for softmax)
        acc = multiclass_accuracy(logits, sentiment_ids,
                                  average='micro',
                                  num_classes=self.num_classes,
                                  ignore_index=-1,
                                  top_k=1)
        
        # Log metrics
        self.log('val/loss', loss, prog_bar=True, sync_dist=True)
        self.log('val/accuracy', acc, prog_bar=True, sync_dist=True)
        
        return loss
    
    def test_step(self, batch, batch_idx):
        """Test step."""
        embeddings = batch['eeg_emb_vector']
        sentiment_ids = batch['sentiment_ids']
        sentiment_ids = sentiment_ids.to(torch.int64) 
        
        # Forward pass
        logits = self(embeddings)
        
        # Calculate loss
        loss = F.cross_entropy(logits, sentiment_ids, ignore_index=-1)
        
        # Calculate accuracy (use logits directly, no need for softmax)
        acc = multiclass_accuracy(logits, sentiment_ids,
                                  average='micro',
                                  num_classes=self.num_classes,
                                  ignore_index=-1,
                                  top_k=1)
        
        # Get predictions
        preds = torch.argmax(logits, dim=1)
        
        # Log metrics
        self.log('test/loss', loss, sync_dist=True)
        self.log('test/accuracy', acc, sync_dist=True)
        
        # Store predictions and targets for confusion matrix
        self.test_step_outputs.append({
            'predictions': preds.detach().cpu(),
            'targets': sentiment_ids.detach().cpu()
        })
        
        return {'loss': loss, 'accuracy': acc, 'predictions': preds, 'targets': sentiment_ids}
    
    def on_test_epoch_end(self):
        """Compute confusion matrix at the end of testing."""
        if len(self.test_step_outputs) == 0:
            return
        
        # Concatenate all predictions and targets
        all_preds = torch.cat([x['predictions'] for x in self.test_step_outputs])
        all_targets = torch.cat([x['targets'] for x in self.test_step_outputs])
        
        # Compute confusion matrix
        cm = confusion_matrix(all_targets.numpy(), all_preds.numpy(), labels=[0, 1, 2])
        
        # Store confusion matrix for later display (will be accessed in train_mlp.py)
        self.confusion_matrix = cm
        
        # Clear for next test run
        self.test_step_outputs.clear()
    
    def predict_step(self, batch, batch_idx):
        """Prediction step."""
        embeddings = batch['eeg_emb_vector']
        
        # Forward pass
        logits = self(embeddings)
        probs = F.softmax(logits, dim=-1)
        preds = torch.argmax(probs, dim=1)
        
        # Convert predictions to labels
        pred_labels = [self.sentiment_labels[pred.item()] for pred in preds]
        
        return {'predictions': preds, 'probabilities': probs, 'labels': pred_labels}
    
    def configure_optimizers(self):
        """Configure optimizer."""
        optimizer = torch.optim.Adam(
            self.parameters(),
            lr=self.lr,
            weight_decay=self.weight_decay
        )
        
        # Learning rate scheduler
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='min',
            factor=0.5,
            patience=5
        )
        
        return {
            'optimizer': optimizer,
            'lr_scheduler': {
                'scheduler': scheduler,
                'monitor': 'val/loss',
                'interval': 'epoch',
                'frequency': 1
            }
        }