# MLP Sentiment Classifier for GLIM Embeddings

This module provides a simple Multi-Layer Perceptron (MLP) classifier that uses embeddings generated by the GLIM model to predict sentiment labels.

## Overview

The MLP sentiment classifier is a lightweight alternative to the full GLIM model for sentiment classification. It works in two stages:

1. **Extract embeddings**: Use a pre-trained GLIM model to generate EEG embeddings
2. **Train MLP**: Train a simple MLP classifier on these embeddings for sentiment prediction

## Simplified Embedding Extraction

GLIM now includes a simplified `extract_embeddings()` method that requires minimal setup:

```python
# Load model
model = GLIM.load_from_checkpoint('checkpoint.ckpt')
model.eval()

# Extract embeddings - that's it!
embeddings = model.extract_embeddings(eeg_data, eeg_mask)

# Optional: with custom prompts
prompts = [('task1', 'ZuCo1', 'ZAB'), ...]
embeddings = model.extract_embeddings(eeg_data, eeg_mask, prompts)
```

**Benefits:**
- ✅ No need to call `setup()` method
- ✅ No need for target text or labels
- ✅ No complex batch dictionary handling
- ✅ No tokenization required
- ✅ Just provide EEG data and mask!

See `example_extract_embeddings.py` for complete examples.

## Model Architecture

The `SentimentMLP` model consists of:
- **Input layer**: Takes GLIM embeddings (default: 1024 dimensions)
- **Hidden layers**: Configurable MLP layers with BatchNorm, ReLU, and Dropout (default: [512, 256])
- **Output layer**: 3-class sentiment prediction (negative, neutral, positive)

## Usage

### 1. Train GLIM Model First

First, train or obtain a pre-trained GLIM model:

```bash
python train.py --data_path ./data/tmp/zuco_merged.df --max_epochs 100
```

This will save checkpoints in `./checkpoints/sentiment_classification/`

### 2. Train MLP Classifier

The training script now uses the simplified `extract_embeddings()` method:

```bash
python train_mlp.py \
  --data_path ./data/tmp/zuco_merged.df \
  --glim_checkpoint ./checkpoints/sentiment_classification/best_model.ckpt \
  --hidden_dims 512 256 \
  --dropout 0.3 \
  --lr 1e-3 \
  --max_epochs 50 \
  --early_stopping \
  --patience 10
```

### Command Line Arguments

**Data Arguments:**
- `--data_path`: Path to merged dataset (required)
- `--glim_checkpoint`: Path to pre-trained GLIM checkpoint (required)

**Model Arguments:**
- `--hidden_dims`: Hidden layer dimensions (default: [512, 256])
- `--dropout`: Dropout rate (default: 0.3)

**Training Arguments:**
- `--batch_size`: Training batch size (default: 64)
- `--val_batch_size`: Validation batch size (default: 32)
- `--lr`: Learning rate (default: 1e-3)
- `--weight_decay`: Weight decay (default: 1e-4)
- `--max_epochs`: Maximum epochs (default: 50)

**Logging Arguments:**
- `--log_dir`: TensorBoard log directory (default: ./logs_mlp)
- `--experiment_name`: Experiment name (default: sentiment_mlp)
- `--checkpoint_dir`: Checkpoint directory (default: ./checkpoints_mlp)

**Hardware Arguments:**
- `--accelerator`: Hardware accelerator (default: auto)
- `--devices`: Number of devices (default: 1)
- `--precision`: Training precision (default: 32)

**Other Arguments:**
- `--early_stopping`: Enable early stopping
- `--patience`: Early stopping patience (default: 10)
- `--seed`: Random seed (default: 42)
- `--num_workers`: Data loading workers (default: 4)

### View Training Progress

Monitor training with TensorBoard:

```bash
tensorboard --logdir ./logs_mlp
```

Open your browser at: http://localhost:6006

## Advantages of MLP Classifier

1. **Faster Training**: Much faster than training full GLIM model
2. **Less Memory**: Requires significantly less GPU memory
3. **Simple Architecture**: Easy to understand and modify
4. **Transfer Learning**: Leverages pre-trained GLIM embeddings
5. **Good Performance**: Can achieve competitive accuracy with proper tuning

## Model Details

### `SentimentMLP` Class

Located in `model/sentiment_mlp.py`

**Key Methods:**
- `forward(embeddings)`: Forward pass through MLP
- `training_step()`: Training step with loss and accuracy logging
- `validation_step()`: Validation step
- `test_step()`: Test step with predictions
- `predict_step()`: Prediction step returning labels
- `configure_optimizers()`: Adam optimizer with ReduceLROnPlateau scheduler

### Training Process

The `train_mlp.py` script:

1. Loads pre-trained GLIM model
2. Freezes GLIM parameters
3. Extracts embeddings for train/val/test sets
4. Trains MLP classifier on embeddings
5. Saves best models based on validation accuracy
6. Evaluates on test set

## Example Workflow

```bash
# 1. Run preprocessing (if not done)
cd data
python preprocess_mat.py
python preprocess_gen_lbl.py  
python preprocess_merge.py
cd ..

# 2. Train GLIM model (or use existing checkpoint)
python train.py --max_epochs 100 --early_stopping

# 3. Train MLP classifier
python train_mlp.py \
  --glim_checkpoint ./checkpoints/sentiment_classification/last.ckpt \
  --hidden_dims 512 256 128 \
  --max_epochs 50 \
  --early_stopping

# 4. View results
tensorboard --logdir ./logs_mlp
```

## Expected Performance

With default hyperparameters, you can expect:
- Training time: ~5-10 minutes (depending on dataset size and hardware)
- Validation accuracy: ~75-85% (depending on GLIM embedding quality)
- Test accuracy: Similar to validation accuracy

## Tips for Better Performance

1. **Tune hidden dimensions**: Try different architectures like [1024, 512] or [512, 256, 128]
2. **Adjust dropout**: Increase for overfitting (0.4-0.5) or decrease for underfitting (0.1-0.2)
3. **Learning rate**: Try different values (1e-4 to 1e-2) with learning rate finder
4. **Batch size**: Larger batches (128-256) can improve stability
5. **Data augmentation**: Use GLIM with different prompts or embeddings
6. **Ensemble**: Train multiple MLPs and ensemble predictions

## Files

- `model/sentiment_mlp.py`: MLP model implementation
- `train_mlp.py`: Training script
- `logs_mlp/`: TensorBoard logs (created during training)
- `checkpoints_mlp/`: Model checkpoints (created during training)
